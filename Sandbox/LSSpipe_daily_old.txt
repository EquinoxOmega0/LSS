#LSS to run with every MTL update
#from the LSS/scripts/main directory

source /global/common/software/desi/desi_environment.sh main
export LSSCODE=<wherever_you_install_the_LSS_repo>
cd $LSSCODE
#if you have not installed it
git clone https://github.com/desihub/LSS.git
cd LSS
git pull
PYTHONPATH=$PYTHONPATH:$LSSCODE/LSS/py
module swap fiberassign/5.0.0

#if you need to delete temporary fiberassign files
rm /global/cfs/cdirs/desi/survey/catalogs//main/LSS/random*/*tmp*

#data, run separately for dark and bright time
#this script first goes through the fiberassign files, first combining all of the information together split by healpix
#all "potential assignments" are kept, meaning each targetid will appear many times
#then, it collects the new spec data for any archived/zdone tiles (incl. the info in the zmtl files used to decide mtl updates)
#then, it combines data from the healpix files selecting on given target types + notqso combinations; notqso means that targets passing the qso selection are removed
#for this combination, only data at the tileids/fiberids that were considered good are kept
#after combining, the sample is matched to the spec data based on 'TARGETID','LOCATION','TILEID','TILELOCID'
#information on the unique groupings of tileids and tilelocid are determined (along with the number of tiles) and this information is joined by a TARGETID match
#It will take awhile, >~ 1 hour for dark time and ~> 30 min for bright, with some variance depending on the number of new tiles (test run with 15 new dark and 41 new bright on Perlmutter)
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --prog dark 
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --prog bright 


#combine quasars:
#run time is linear in number of tiles to process
#some number, missing files not necessary for redrock pipeline but necessary for quasar catalog, will fail
#if number of failures is > 4 (number failing for Y1), notify people so we can keep track
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --prog dark --doqso y --dospec n --combpix n

#to make the emission line files for each tile
#run time is ~linear in number of tiles to process, < 100 should take < 1 hr
srun -N 1 -C cpu -t 02:00:00 -q interactive python $LSSCODE/LSS/scripts/mkemlin.py

#to combine the emission line files:
python $LSSCODE/LSS/scripts/main/combdata_main.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --prog dark --mkemlin y --dospec n --combpix n


#the below script is run for this list of target types + notqso combinations in order to generate the "full" LSS catalogs, pre veto masks
#in this step, only unique targetid are kept, prioritizing those with an observation and then those with the greatest tsnr2
#targets at tileids/fiberid where none of the given type were assigned are masked
#if enhanced information on qso or ELG targets exists, it is addedca
#completeness statistics per tile grouping ('COMP_TILE') and per tileid/fiberid ('FRACZTILELOCID') are calculated
#running this script will go through all, takes ~1 hr
$LSSCODE/LSS/scripts/main/daily_main_data_full.sh
#or go through one by one
for tp,notqso in zip(tps,notqsos):
    python $LSSCODE/LSS/scripts/main/mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld y --verspec daily --notqso notqso


#random
#first, we go through bright and dark, making the mtl files per tile that fiberassign uses, then running fiberassign (to get potential assignments, which is the FAVAIL HDU in the fba files)
#after fiberassign is run, the potential assignments are combined per healpix
#does 18 randoms in parallel
#total amount of time is ~linear in the number of tiles to run
srun -N 1 -C cpu -t 04:00:00 -q interactive python $LSSCODE/LSS/scripts/main/mkCat_main_ran_px.py  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --type dark --ranmtl y 
srun -N 1 -C cpu -t 04:00:00 -q interactive python $LSSCODE/LSS/scripts/main/mkCat_main_ran_px.py  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --type bright --ranmtl y 

#if times out while still running fiberassign files, make sure to delete files with .tmp in the name

#then, we go through per type
#the "full" random files are made for each healpix
#this masks them using the same imaging mask bits as applied to targeting and also removes the tileids/fiberid where none of the given type were assigned (but an observation was requested)
for tp,notqso in zip(tps,notqsos):
    #on cori
   
    salloc -N 2 -C haswell -t 04:00:00 --qos interactive --account desi
    ./daily_main_randoms_type_2nodes_noveto.sh tp notqso
    #or run all
    ./daily_main_randoms_all_2nodes_noveto.sh
    #on perlmutter, enough memory so one node fine?
    #srun -N 1 -C cpu -t 04:00:00 -q interactive python mkCat_main_ran_px.py  --type tp  --basedir /global/cfs/cdirs/desi/survey/catalogs/ --verspec daily --rfa n --combhp n --combfull y --fullr y 
#if data model is stable since the last updates, add to the arguments
    --refullr n

#apply veto mask column for LRGs
#!don't need this anymore!
#python getLRGmask.py --basedir /global/cfs/cdirs/desi/survey/catalogs/ --survey main --verspec daily --maxr 18

#this adds vetos to both data and randoms (could put randoms in separate script and parallize)
#only necessary for LRGs for now
for tp,notqso in zip(tps,notqsos):
    python mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --add_veto y --verspec daily --notqso notqso
#can just run one random and then add --maxr 1

#this applies vetos to both data and randoms (could put randoms in separate script and parallize)

for tp,notqso in zip(tps,notqsos):
    python mkCat_main.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n --apply_veto y --verspec daily --notqso notqso
#can just run one random and then add --maxr 1
    
#to do randoms in parallel:
    python mkCat_main_ran.py --type tp --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fullr n --apply_veto y --verspec daily --notqso notqso

#get k+e
python mkCat_main.py --type BGS_BRIGHT --verspec daily --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n  --add_ke y --survey Y1
    
#get regressis weights
for tp,notqso in zip(tps,notqsos):
	python mkCat_main.py --type tp  --basedir /global/cfs/cdirs/desi/survey/catalogs/  --fulld n  --regressis y --add_regressis y --verspec daily --notqso notqso
    
